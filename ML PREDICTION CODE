#IMPORTING ALL THE LIBRARIES

from sklearn.metrics import confusion_matrix ,classification_report , precision_score, recall_score, f1_score, roc_auc_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

!pip install shap==0.40.0
import shap
import graphviz
sns.set_style('darkgrid')




ct = pd.crosstab(dataset['wifi'],dataset['price_range'])
from scipy.stats import chi2_contingency
stat,pvalue,dof,expected_R = chi2_contingency(ct)
print("pvalue:",pvalue)

if pvalue <= 0.1:
  print("Alternate Hypothesis passed. int_memory and price_range have Relationship")
else:
  print("Null hypothesis passed. int_memory and price_range doesnot have Relation")
  
  
  
  # defining new variable for pixels
dataset['pixels'] = dataset['px_height'] * dataset['px_width']

# dropping px_height and px_width
dataset.drop(['px_height', 'px_width'], axis=1, inplace=True)



#defining X and Y

x = dataset.drop(['price_range'], axis = 1).values
y = dataset['price_range'].values


#scaling the value of X

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x = scaler.fit_transform(x)



#SEPERATE THE DATA SET IN TWO TYPE ON TRAINING IS 75% OF DATA AND OTHER TESTING IS 25% OF THE DATA 

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=167)
print(x_train.shape)
print(x_test.shape)


# applying logistic regression
logmodel = LogisticRegression()
logmodel.fit(x_train, y_train)


#predict on the model
#get the predicted probabilities 

trainscore = logmodel.score(x_train,y_train)
testscore = logmodel.score(x_test,y_test)

print("train score : {} ".format(trainscore),'\n')
print("test score : {} ".format(testscore),'\n')

y_predlogi = logmodel.predict(x_test)
print("f1 score :",f1_score(y_test,y_predlogi,average='micro'),'\n')

#get the confusion matrix for both the train and test

print(confusion_matrix(y_test,y_predlogi))


print("precision score :",precision_score(y_test,y_predlogi,average='micro'),'\n')
print("recall score :",recall_score(y_test,y_predlogi,average='micro'),'\n')
print(classification_report(y_test, y_predlogi))



#----------logistic regression----------

probabilityvalues = logmodel.predict_proba(x)
auc = roc_auc_score(y,probabilityvalues,multi_class ='ovr')
print(auc)



#ML MODULE-1  IMPLEMANTING HYPERPARAMETER OPTIMIZING TECHNIQUES

param ={'C':[0.01,0.1,1,10,100,110,120,130]}
grid = GridSearchCV(LogisticRegression(max_iter=500),param,n_jobs=-1)
grid.fit(x_train,y_train)



#predict on the model 
#get the predicted class

trainscore = grid.score(x_train,y_train)
testscore = grid.score(x_test,y_test)
print("train score : {} ".format(trainscore),'\n')
print("test score : {} ".format(testscore),'\n')

y_predlogi = logmodel.predict(x_test)
print("f1 score :",f1_score(y_test,y_predlogi,average='micro'),'\n')

#get the confusion matrix for both the train and test

print(confusion_matrix(y_test,y_predlogi))



print("precision score :",precision_score(y_test,y_predlogi,average ='micro'),'\n')
print("recall score :", recall_score(y_test,y_predlogi, average='micro'),'\n')
print(classification_report(y_test,y_predlogi))



#Applying XGBoost

from xgboost import XGBClassifier
xgb = XGBClassifier(max_depth = 5, learning_rate=0.1)
xgb.fit(x_train,y_train)
XGBClassifier(max_depth=5, objective ='multi:softprob')



#prediction 

y_pred_train = xgb.predict(x_train)
y_pred_test = xgb.predict(x_test)

#evaluation metrics for test 

print('train_score:',accuracy_score(y_train,y_pred_train),'\n')
print('test_score:',accuracy_score(y_test,y_pred_test),'\n')
print('precision score:', precision_score(y_test,y_pred_test,average='micro'),'\n')
print('recall score:',recall_score(y_test,y_pred_test,average='micro'),'\n')
score = classification_report(y_test,y_pred_test)
print('classification Report for XGboost(test set)=')
print(score)


#cross validation 

from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(xgb, param_grid={'n_estimators':(10,200), 'learning_rate': [1,0.5,0.1,0.01,0.001], 'max_depth':(5,10),
                                     'gamma': [1.5,1.8], 'subsample':[0.3,0.5,0.8]}, cv=5, scoring='accuracy', verbose=10)
grid.fit(x_train,y_train)


#find best params value 
grid.best_params_

#find best estimeter 
grid.best_estimator_

#applying best etimeter value in xgboost
model = XGBClassifier(gamma=1.5,max_depth=10,n_estimator=200,
                      objective='multi:softprob', subsample=0.5)
model.fit(x_train,y_train)


#predicit on the model 
#get the predicted probability 

trainscore = model.score(x_train,y_train)
testscore = model.score(x_test,y_test)
print("train score: {}".format(trainscore),'\n')
print("test score: {}".format(testscore),'\n')
y_predxgb = model.predict(x_test)
print('f1 score:', f1_score(y_test,y_predxgb,average='micro'),'\n')



#evaluation metrics for train
print('precision score:', precision_score(y_test,y_predxgb,average='micro'),'\n')
print('recall score:',recall_score(y_test,y_predxgb,average='micro'),'\n')
score= classification_report(y_train,y_pred_train)
print('classification Report for tuned XGBoost(Train set)=')
print(score)


#----------XGBoosting----------

probabilityvalues = RFmodel.predict_proba(x)
auc = roc_auc_score(y,probabilityvalues,multi_class='ovr')
print(auc)



